{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "driver = webdriver.Chrome('/Users/robjohns/Documents/Metis/Project2/chromedriver')\n",
    "URL=super_short_test_list[0]\n",
    "driver.get(URL)\n",
    "time.sleep(5)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "driver.quit()\n",
    "print(len(soup.find_all('div', attrs={'class': 'gws-flights-results__itinerary-duration'})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/flights?hl=en#flt=SEA.SFO.2019-07-22;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o\n"
     ]
    }
   ],
   "source": [
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how to avoid booting from server ideas . . . .\n",
    "for page in super_short_test_list:\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    #time.sleep(.5+2*random.random())\n",
    "    \n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://www.reddit.com'\n",
    "\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "response  = requests.get(url, headers = user_agent)\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "print(user_agent)\n",
    "\n",
    "response  = requests.get(url, headers = user_agent)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of flights: 30\n",
      "\n",
      "[['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.03, 236.0, '6:00 A', '8:02 A', 'United', 'UA368', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.17, 236.0, '3:45 P', '5:55 P', 'Delta', 'DL2787', 'Boeing 717'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.32, 236.0, '9:25 P', '11:44 P', 'Alaska', 'AS1330', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.42, 236.0, '5:50 A', '8:15 A', 'Alaska', ('REGEXerror:', 'Embraer RJ-175AS\\xa02950'), ('REGEXerror:', 'Embraer RJ-175AS\\xa02950')], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.18, 236.0, '6:00 A', '8:11 A', 'Delta', 'DL362', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.38, 236.0, '6:50 A', '9:13 A', 'United', 'UA659', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.35, 236.0, '7:40 A', '10:01 A', 'Delta', 'DL2490', 'Boeing 717'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.38, 236.0, '9:25 A', '11:48 A', 'Delta', 'DL2429', 'Boeing 717'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.42, 236.0, '9:35 A', '12:00 P', 'Alaska', 'AS1744', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.42, 236.0, '10:55 A', '1:20 P', 'Alaska', 'AS304', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.27, 236.0, '11:15 A', '1:31 P', 'Delta', 'DL1470', 'Airbus A319'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.42, 236.0, '11:50 A', '2:15 P', 'Alaska', 'AS1748', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.22, 236.0, '1:45 P', '3:58 P', 'Delta', 'DL2002', 'Boeing 717'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.33, 236.0, '2:25 P', '4:45 P', 'Alaska', 'AS1752', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.33, 236.0, '3:30 P', '5:50 P', 'Alaska', 'AS1754', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.18, 236.0, '4:05 P', '6:16 P', 'United', 'UA1451', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.18, 236.0, '5:15 P', '7:26 P', 'United', 'UA587', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.2, 236.0, '6:35 P', '8:47 P', 'United', 'UA214', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.27, 236.0, '6:59 P', '9:15 P', 'Delta', 'DL1190', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.33, 236.0, '7:25 P', '9:45 P', 'Alaska', 'AS376', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.25, 236.0, '7:40 P', '9:55 P', 'United', 'UA2070', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.33, 236.0, '8:20 P', '10:40 P', 'Alaska', 'AS1934', 'Airbus A320'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.28, 236.0, '9:34 P', '11:51 P', 'Delta', 'DL1657', 'Boeing 717'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.08, 238.0, '5:12 P', '7:17 P', 'Delta', 'DL856', 'Boeing 717'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.42, 259.0, '6:00 A', '8:25 A', 'Alaska', 'AS766', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.42, 299.0, '7:50 A', '10:15 A', 'Alaska', 'AS405', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.32, 299.0, '9:55 A', '12:14 P', 'United', 'UA698', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.32, 299.0, '11:33 A', '1:52 P', 'United', 'UA2236', 'Boeing 757'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.32, 299.0, '1:10 P', '3:29 P', 'United', 'UA816', 'Boeing 737'], ['https://www.google.com/flights?hl=en#flt=SAN.SEA.2019-12-31;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o', '2019-12-31', 'SAN', 'SEA', 2.33, 299.0, '5:25 P', '7:45 P', 'Alaska', 'AS1756', 'Airbus A320']]\n"
     ]
    }
   ],
   "source": [
    "##this will take the soupy html file and pull out data values for all flight results from a search##\n",
    "# looking for [Date,DepartureCity,ArrivalCity,DepartureTime,\n",
    "#               ArrivalTime,FlightDuration,Price,PlaneType,Airline,FlightNumber]\n",
    "import re\n",
    "#'https://www.google.com/flights?hl=en#flt=SFO.AUS.2019-08-21;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "#each page has a different number of flights on it, so for this URL we need to make a list of lists\n",
    "#that is the length = number of flights on the page#\n",
    "\n",
    "Locations = re.search('#flt=(.+?);b:1', URL).group(1)\n",
    "Spots=Locations.split('.')\n",
    "Departure=Spots[0]\n",
    "Arrival = Spots[1]\n",
    "Date= Spots[2]\n",
    "\n",
    "\n",
    "\n",
    "flights=len(soup.find_all('div', attrs={'class': 'gws-flights-results__itinerary-duration'}))\n",
    "print('num of flights:',flights)\n",
    "BigList = [[] for _ in range(flights)]\n",
    "for flight in BigList:\n",
    "    flight.append(URL)\n",
    "    flight.append(Date)\n",
    "    flight.append(Departure)\n",
    "    flight.append(Arrival)\n",
    "    \n",
    "# Append flight durations onto each element of the list\n",
    "i=-1\n",
    "for li in soup.find_all('div', attrs={'class': 'gws-flights-results__duration flt-subhead1Normal'}):\n",
    "    i+=1\n",
    "    chonk=li.get_text()\n",
    "    try:\n",
    "        ch=chonk.split('h')\n",
    "        onk=chonk.split(' ')\n",
    "        num2=onk[1]\n",
    "        flight_duration=float(ch[0])+(float(num2[:-1])/60)\n",
    "        flight_duration = round(flight_duration,2)\n",
    "        BigList[i].append(flight_duration)\n",
    "        #print ('flight_duration:',flight_duration)\n",
    "    except:\n",
    "        BigList[i].append('NaN')\n",
    "        print('There was an issue')\n",
    "\n",
    "\n",
    "\n",
    "#for li in soup.find_all('div', attrs={'class': 'gws-flights-results__price'}):\n",
    "    #print(li)\n",
    "\n",
    "\n",
    "#here we add more flight info from the jstcache. . . beware this number seems to change from day to day\n",
    "i=-1\n",
    "for li in soup.find_all('jsl', attrs={'jstcache': \"8836\"}):\n",
    "    i+=1\n",
    "    text = li.get_text()\n",
    "    \n",
    "      #finding price\n",
    "    try:\n",
    "        found = re.search('From (.+?)Trip', text).group(1)\n",
    "        dollar=found.replace(',','')\n",
    "        found=float(dollar[1:-1])\n",
    "    except AttributeError:\n",
    "    # if the sentence structure i didnt expect show up\n",
    "        found = 'NaN' # apply your error handling\n",
    "    #print('cost:',found)\n",
    "    BigList[i].append(found)\n",
    "    #finding departure and arrival times\n",
    "    try:\n",
    "        found = re.search('Departure time: (.+?)M', text).group(1)\n",
    "        found2 = re.search('Arrival time: (.+?)M', text).group(1)\n",
    "        \n",
    "        #departure=found.replace(',','')\n",
    "        #found=float(dollar[1:-1])\n",
    "    except AttributeError:\n",
    "    # if the sentence structure i didnt expect show up\n",
    "        found = 'NaN' # apply your error handling\n",
    "    #print('Departure Time:',found)\n",
    "    #print('Arrival Time:',found2)\n",
    "    BigList[i].append(found)\n",
    "    BigList[i].append(found2)\n",
    "    \n",
    "  \n",
    "    #grab airline\n",
    "    try:\n",
    "        found = re.search('by (.+?).Depa', text).group(1)\n",
    "        \n",
    "        #departure=found.replace(',','')\n",
    "        #found=float(dollar[1:-1])\n",
    "    except AttributeError:\n",
    "    # if the sentence structure i didnt expect show up\n",
    "        found = 'NaN' # apply your error handling\n",
    "    #print('Airline:',found)\n",
    "    BigList[i].append(found)\n",
    "print('')\n",
    "    #grab PlaneType and flight number\n",
    "i=-1    \n",
    "for li in soup.find_all('div', attrs={'class': 'gws-flights-results__other-leg-info'}):\n",
    "        i+=1\n",
    "        #print(li.get_text())\n",
    "        st=li.get_text()\n",
    "        #st=str(st.text())\n",
    "        result = re.match(\"(?P<PlaneType>[A-Za-z]+.[A-Za-z]*[0-9]+)(?P<FlightNum>[A-Za-z]+\\s[0-9]+)\", st)\n",
    "        if result:\n",
    "            Planetype=result.group('PlaneType')\n",
    "            Flightnumber=result.group('FlightNum')\n",
    "            Flightnumber=Flightnumber.replace('\\xa0','')\n",
    "            #print('Flightnum:',Flightnumber)\n",
    "            #print('Planetype:',Planetype)\n",
    "            BigList[i].append(Flightnumber)\n",
    "            BigList[i].append(Planetype)\n",
    "            #print(\"\")\n",
    "        else:\n",
    "            errormsg=('REGEXerror:',st)\n",
    "            BigList[i].append(errormsg)\n",
    "            BigList[i].append(errormsg)\n",
    "\n",
    "print(BigList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super short test set has this many values:8\n",
      "test set has this many values:18\n",
      "full city set has this many values:132\n",
      "full date set has this many values:652\n",
      "full set has this many values:1956\n",
      "come home set has this many values:1956\n"
     ]
    }
   ],
   "source": [
    "##Format URLs, can create these lists:      super_short_test_list//test_urls//full_cities_lessdates_urls//\n",
    "##//full_dates_lesscities_urls//full_url_list//come_home_URLS##\n",
    "\n",
    "cities=['SFO','AUS','ATL','BOI','BOS','CUN','ORD','DEN','DTW','FAI','JFK','SAN']\n",
    "\n",
    "##links look like this for one ways,nonstop, with overhead bags allowed, and frontier airlines excluded##\n",
    "'https://www.google.com/flights?hl=en#flt=SFO.AUS.2019-08-21;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "\n",
    "#This makes all the dates from july 22nd to the end of the year, will update from date time later if i have time\n",
    "dates=['2019-07-22']\n",
    "for days in range(23,32):\n",
    "    date = '2019-07-' + str(days)\n",
    "    dates.append(date)\n",
    "for days in range(1,32):\n",
    "    if days>=10:\n",
    "        date = '2019-08-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-08-0' + str(days)\n",
    "        dates.append(date)\n",
    "for days in range(1,31):\n",
    "    if days>=10:\n",
    "        date = '2019-09-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-09-0' + str(days)\n",
    "        dates.append(date)\n",
    "for days in range(1,32):\n",
    "    if days>=10:\n",
    "        date = '2019-10-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-10-0' + str(days)\n",
    "        dates.append(date) \n",
    "\n",
    "for days in range(1,31):\n",
    "    if days>=10:\n",
    "        date = '2019-11-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-11-0' + str(days)\n",
    "        dates.append(date)           \n",
    "\n",
    "for days in range(1,32):\n",
    "    if days>=10:\n",
    "        date = '2019-12-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-12-0' + str(days)\n",
    "        dates.append(date)         \n",
    "\n",
    "## This makes lists of URLS to use generating data##\n",
    "super_short_test_list=[]\n",
    "super_short_test_list_dates=[]\n",
    "for city in cities[::6]:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates[::43]:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        super_short_test_list.append(URL)\n",
    "        super_short_test_list_dates.append(date)\n",
    "        \n",
    "print('super short test set has this many values:'+str(len(super_short_test_list)))\n",
    "\n",
    "test_urls=[]\n",
    "for city in cities[::5]:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates[::29]:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        test_urls.append(URL)\n",
    "print('test set has this many values:'+str(len(test_urls)))        \n",
    "        \n",
    "full_cities_lessdates_urls=[]\n",
    "for city in cities:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates[::15]:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        full_cities_lessdates_urls.append(URL)\n",
    "print('full city set has this many values:'+str(len(full_cities_lessdates_urls)))   \n",
    "\n",
    "full_dates_lesscities_urls=[]\n",
    "for city in cities[::3]:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        full_dates_lesscities_urls.append(URL)\n",
    "print('full date set has this many values:'+str(len(full_dates_lesscities_urls)))  \n",
    "\n",
    "full_url_list=[]\n",
    "for city in cities:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        full_url_list.append(URL)\n",
    "print('full set has this many values:'+str(len(full_url_list)))  \n",
    "\n",
    "come_home_URLS=[]\n",
    "for city in cities:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt='+city+'.'+'SEA.'\n",
    "    for date in dates:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        come_home_URLS.append(URL)\n",
    "print('come home set has this many values:'+str(len(come_home_URLS)))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
