{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta, datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Getting_Flight_Info(URL):\n",
    "    driver = webdriver.Chrome('/Users/robjohns/Documents/Metis/Project2/chromedriver')\n",
    "    driver.get(URL)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    driver.quit()\n",
    "    data=DataGetter(URL,soup)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of flights: 6\n",
      "\n",
      "                                                  0           1    2    3   \\\n",
      "0  https://www.google.com/flights?hl=en#flt=SFO.A...  2019-08-21  SFO  AUS   \n",
      "1  https://www.google.com/flights?hl=en#flt=SFO.A...  2019-08-21  SFO  AUS   \n",
      "2  https://www.google.com/flights?hl=en#flt=SFO.A...  2019-08-21  SFO  AUS   \n",
      "3  https://www.google.com/flights?hl=en#flt=SFO.A...  2019-08-21  SFO  AUS   \n",
      "4  https://www.google.com/flights?hl=en#flt=SFO.A...  2019-08-21  SFO  AUS   \n",
      "5  https://www.google.com/flights?hl=en#flt=SFO.A...  2019-08-21  SFO  AUS   \n",
      "\n",
      "     4      5        6        7       8       9            10  \n",
      "0  3.75   79.0   9:20 A   3:05 P  Alaska  AS1218  Airbus A320  \n",
      "1  3.67   79.0   6:45 P  12:25 A  Alaska  AS1222  Airbus A320  \n",
      "2  3.58  166.0   7:40 A   1:15 P  United   UA258   Boeing 737  \n",
      "3  3.67  166.0  10:50 A   4:30 P  United   UA701  Airbus A320  \n",
      "4  3.55  166.0  11:55 P   5:28 A  United  UA1471  Airbus A319  \n",
      "5  3.55  199.0   4:45 P  10:18 P  United  UA2358  Airbus A319  \n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "\n",
    "URL='https://www.google.com/flights?hl=en#flt=SFO.AUS.2019-08-21;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "data=Getting_Flight_Info(URL)\n",
    "data=pd.DataFrame(data)\n",
    "data.to_pickle('Picklefolder/testpickle')\n",
    "\n",
    "\n",
    "testy=pd.read_pickle('Picklefolder/testpickle')\n",
    "print(testy)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how to avoid booting from server ideas . . . .\n",
    "for page in super_short_test_list:\n",
    "    ### scrape a website\n",
    "    ### ...\n",
    "    print(page)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    #time.sleep(.5+2*random.random())\n",
    "    \n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://www.reddit.com'\n",
    "\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "response  = requests.get(url, headers = user_agent)\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "print(user_agent)\n",
    "\n",
    "response  = requests.get(url, headers = user_agent)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##this will take the soupy html file and pull out data values for all flight results from a search##\n",
    "# looking for [Date,DepartureCity,ArrivalCity,DepartureTime,\n",
    "#               ArrivalTime,FlightDuration,Price,PlaneType,Airline,FlightNumber]\n",
    "import re\n",
    "#'https://www.google.com/flights?hl=en#flt=SFO.AUS.2019-08-21;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "#each page has a different number of flights on it, so for this URL we need to make a list of lists\n",
    "#that is the length = number of flights on the page#\n",
    "\n",
    "\n",
    "def DataGetter(URL,soup):\n",
    "\n",
    "    Locations = re.search('#flt=(.+?);b:1', URL).group(1)\n",
    "    Spots=Locations.split('.')\n",
    "    Departure=Spots[0]\n",
    "    Arrival = Spots[1]\n",
    "    Date= Spots[2]\n",
    "\n",
    "\n",
    "\n",
    "    flights=len(soup.find_all('div', attrs={'class': 'gws-flights-results__itinerary-duration'}))\n",
    "    print('num of flights:',flights)\n",
    "    BigList = [[] for _ in range(flights)]\n",
    "    for flight in BigList:\n",
    "        flight.append(URL)\n",
    "        flight.append(Date)\n",
    "        flight.append(Departure)\n",
    "        flight.append(Arrival)\n",
    "\n",
    "    # Append flight durations onto each element of the list\n",
    "    i=-1\n",
    "    for li in soup.find_all('div', attrs={'class': 'gws-flights-results__duration flt-subhead1Normal'}):\n",
    "        i+=1\n",
    "        chonk=li.get_text()\n",
    "        try:\n",
    "            ch=chonk.split('h')\n",
    "            onk=chonk.split(' ')\n",
    "            num2=onk[1]\n",
    "            flight_duration=float(ch[0])+(float(num2[:-1])/60)\n",
    "            flight_duration = round(flight_duration,2)\n",
    "            BigList[i].append(flight_duration)\n",
    "            #print ('flight_duration:',flight_duration)\n",
    "        except:\n",
    "            BigList[i].append('NaN')\n",
    "            print('There was an issue')\n",
    "\n",
    "\n",
    "\n",
    "    #for li in soup.find_all('div', attrs={'class': 'gws-flights-results__price'}):\n",
    "        #print(li)\n",
    "\n",
    "\n",
    "    #here we add more flight info from the jstcache. . . beware this number seems to change from day to day\n",
    "    i=-1\n",
    "    for li in soup.find_all('jsl', attrs={'jstcache': \"8836\"}):\n",
    "        i+=1\n",
    "        text = li.get_text()\n",
    "\n",
    "          #finding price\n",
    "        try:\n",
    "            found = re.search('From (.+?)Trip', text).group(1)\n",
    "            dollar=found.replace(',','')\n",
    "            found=float(dollar[1:-1])\n",
    "        except AttributeError:\n",
    "        # if the sentence structure i didnt expect show up\n",
    "            found = 'NaN' # apply your error handling\n",
    "        #print('cost:',found)\n",
    "        BigList[i].append(found)\n",
    "        #finding departure and arrival times\n",
    "        try:\n",
    "            found = re.search('Departure time: (.+?)M', text).group(1)\n",
    "            found2 = re.search('Arrival time: (.+?)M', text).group(1)\n",
    "\n",
    "            #departure=found.replace(',','')\n",
    "            #found=float(dollar[1:-1])\n",
    "        except AttributeError:\n",
    "        # if the sentence structure i didnt expect show up\n",
    "            found = 'NaN' # apply your error handling\n",
    "        #print('Departure Time:',found)\n",
    "        #print('Arrival Time:',found2)\n",
    "        BigList[i].append(found)\n",
    "        BigList[i].append(found2)\n",
    "\n",
    "\n",
    "        #grab airline\n",
    "        try:\n",
    "            found = re.search('by (.+?).Depa', text).group(1)\n",
    "\n",
    "            #departure=found.replace(',','')\n",
    "            #found=float(dollar[1:-1])\n",
    "        except AttributeError:\n",
    "        # if the sentence structure i didnt expect show up\n",
    "            found = 'NaN' # apply your error handling\n",
    "        #print('Airline:',found)\n",
    "        BigList[i].append(found)\n",
    "    #print('')\n",
    "        #grab PlaneType and flight number\n",
    "    i=-1    \n",
    "    for li in soup.find_all('div', attrs={'class': 'gws-flights-results__other-leg-info'}):\n",
    "            i+=1\n",
    "            #print(li.get_text())\n",
    "            st=li.get_text()\n",
    "            #st=str(st.text())\n",
    "            result = re.match(\"(?P<PlaneType>[A-Za-z]+.[A-Za-z]*[0-9]+)(?P<FlightNum>[A-Za-z]+\\s[0-9]+)\", st)\n",
    "            if result:\n",
    "                Planetype=result.group('PlaneType')\n",
    "                Flightnumber=result.group('FlightNum')\n",
    "                Flightnumber=Flightnumber.replace('\\xa0','')\n",
    "                #print('Flightnum:',Flightnumber)\n",
    "                #print('Planetype:',Planetype)\n",
    "                BigList[i].append(Flightnumber)\n",
    "                BigList[i].append(Planetype)\n",
    "                #print(\"\")\n",
    "            else:\n",
    "                errormsg=('REGEXerror:',st)\n",
    "                BigList[i].append(errormsg)\n",
    "                BigList[i].append(errormsg)\n",
    "\n",
    "    return BigList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for lists in BigList:\n",
    "    print(len(lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super short test set has this many values:8\n",
      "test set has this many values:18\n",
      "full city set has this many values:132\n",
      "full date set has this many values:652\n",
      "full set has this many values:1956\n",
      "come home set has this many values:1956\n"
     ]
    }
   ],
   "source": [
    "##Format URLs, can create these lists:      super_short_test_list//test_urls//full_cities_lessdates_urls//\n",
    "##//full_dates_lesscities_urls//full_url_list//come_home_URLS##\n",
    "\n",
    "cities=['SFO','AUS','ATL','BOI','BOS','CUN','ORD','DEN','DTW','FAI','JFK','SAN']\n",
    "\n",
    "##links look like this for one ways,nonstop, with overhead bags allowed, and frontier airlines excluded##\n",
    "'https://www.google.com/flights?hl=en#flt=SFO.AUS.2019-08-21;b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "\n",
    "#This makes all the dates from july 22nd to the end of the year, will update from date time later if i have time\n",
    "dates=['2019-07-22']\n",
    "for days in range(23,32):\n",
    "    date = '2019-07-' + str(days)\n",
    "    dates.append(date)\n",
    "for days in range(1,32):\n",
    "    if days>=10:\n",
    "        date = '2019-08-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-08-0' + str(days)\n",
    "        dates.append(date)\n",
    "for days in range(1,31):\n",
    "    if days>=10:\n",
    "        date = '2019-09-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-09-0' + str(days)\n",
    "        dates.append(date)\n",
    "for days in range(1,32):\n",
    "    if days>=10:\n",
    "        date = '2019-10-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-10-0' + str(days)\n",
    "        dates.append(date) \n",
    "\n",
    "for days in range(1,31):\n",
    "    if days>=10:\n",
    "        date = '2019-11-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-11-0' + str(days)\n",
    "        dates.append(date)           \n",
    "\n",
    "for days in range(1,32):\n",
    "    if days>=10:\n",
    "        date = '2019-12-' + str(days)\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '2019-12-0' + str(days)\n",
    "        dates.append(date)         \n",
    "\n",
    "## This makes lists of URLS to use generating data##\n",
    "super_short_test_list=[]\n",
    "super_short_test_list_dates=[]\n",
    "for city in cities[::6]:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates[::43]:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        super_short_test_list.append(URL)\n",
    "        super_short_test_list_dates.append(date)\n",
    "        \n",
    "print('super short test set has this many values:'+str(len(super_short_test_list)))\n",
    "\n",
    "test_urls=[]\n",
    "for city in cities[::5]:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates[::29]:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        test_urls.append(URL)\n",
    "print('test set has this many values:'+str(len(test_urls)))        \n",
    "        \n",
    "full_cities_lessdates_urls=[]\n",
    "for city in cities:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates[::15]:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        full_cities_lessdates_urls.append(URL)\n",
    "print('full city set has this many values:'+str(len(full_cities_lessdates_urls)))   \n",
    "\n",
    "full_dates_lesscities_urls=[]\n",
    "for city in cities[::3]:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        full_dates_lesscities_urls.append(URL)\n",
    "print('full date set has this many values:'+str(len(full_dates_lesscities_urls)))  \n",
    "\n",
    "full_url_list=[]\n",
    "for city in cities:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt=SEA.'+city+'.'\n",
    "    \n",
    "    for date in dates:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        full_url_list.append(URL)\n",
    "print('full set has this many values:'+str(len(full_url_list)))  \n",
    "\n",
    "come_home_URLS=[]\n",
    "for city in cities:\n",
    "    URLc='https://www.google.com/flights?hl=en#flt='+city+'.'+'SEA.'\n",
    "    for date in dates:\n",
    "        URL=URLc+date+';b:1;c:USD;e:1;s:0;a:-F9;sd:1;t:f;tt:o'\n",
    "        come_home_URLS.append(URL)\n",
    "print('come home set has this many values:'+str(len(come_home_URLS)))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
